# PoS_21_group12

Assignments for Programming of Supercomunter WiSe 21-22 - Group 12

1.2 **Compiler Options**

All optimization levels except -O0 may affect debugging (-g)

-O0 : Disables all optimizations. Recommended for program development and debugging, GNU default

-O1 : Enables optimization for speed, while being aware of code size (e.g no loop unrolling)

-O2 : Default optimization. Optimizations for speed, including global code scheduling, software pipelining, predication, and speculation. Intel default. Intel -O2 includes function inlining, while GNU does not

-O3 : -O2 optimizations plus more aggressive optimizations such as prefetching, scalar replacement, and loop transformations. Enables optimizations for technical computing applications (loop-intensive code): loop optimizations and data prefetch. more aggressive optimizations than -O2, but not always better

Intel: -O2, -O3 imply inlining

Intel: -ip implies inlining and additional interprocedural optimizations

-Ofast :- Sets certain aggressive options to improve the speed of your application. This option improves the speed of your application. It sets compiler options -O3, -no-prec-div and -fp-model fast=2

-xhost : Tells the compiler to generate instructions for the highest instruction set available on the compilation host processor. This option tells the compiler to generate instructions for the highest instruction set available on the compilation host processor. The instructions generated by this compiler option differ depending on the compilation host processor. The following table describes the effects of specifying the xHost option and it tells whether the resulting executable will run on processors different from the host processor.

Instruction Set of Host Processor	Effects When the -xHost Compiler Option is Specified
Intel® AVX2				            Corresponds to option xCORE-AVX2.
Intel® AVX				            Corresponds to option xAVX.
Intel® SSE4.2			            Corresponds to option xSSE4.2
Intel® SSE4.1			            Corresponds to option xSSE4.1
SSSE3					            Corresponds to option xSSSE3
Intel® SSE3				            Corresponds to option xSSE3
Intel® SSE2				            Corresponds to option -msse2

**1.2.1** What is the meaning of -ipo? Enable multi-file IP optimizations (between files). 	Enables interprocedural (IP) optimizations, e.g. inline function expansion for calls to functions defined in separate files

Enables interprocedural optimization between files.
Arguments
n
Is an optional integer that specifies the number of object files the compiler should create. The integer must be greater than or equal to 0.

Default: -no-ipo. Multifile interprocedural optimization is not enabled.

This option enables interprocedural optimization between files. This is also called multifile interprocedural optimization (multifile IPO) or Whole Program Optimization (WPO).
When you specify this option, the compiler performs inline function expansion for calls to functions defined in separate files.
You cannot specify the names for the files that are created.
If n is 0, the compiler decides whether to create one or more object files based on an estimate of the size of the application. It generates one object file for small applications, and two or more object files for large applications.
If n is greater than 0, the compiler generates n object files, unless n exceeds the number of source files (m), in which case the compiler generates only m object files.
If you do not specify n, the default is 0.

**1.2.2** What is the meaning of -fno-alias? Assume no aliasing in program. Specifies that aliasing should not be assumed in the program. Allows the compiler to generate faster code.

Aliasing means, that a memory address can be accessed by different symbolic names (variables, pointers)
Aliasing prohibits optimizations, e.g.:
x = 1;
*p = 42;
y = 2 * pi * x;
– Compiler could propagate x = 1 to last line
– But wait, p could be a pointer to x!
You should tell the compiler to what aliasing rules your code conforms
If code does not conform to the rules: unexpected results

ISO C defines rule for “strict aliasing”
– Pointers of different type must not alias each other
Compilers may rely on this rule at higher optimization levels
– GNU: -O2 enables -fstrict-aliasing
– Intel: even -O3 does not enable strict aliasing, do this with -ansi-alias or -fstrict-aliasing
You can define even more strict aliasing rules
– Function arguments do not alias each other, even if same type: -fargument-noalias
– Additionally, arguments do not alias global storage: -fargument-noalias-global
– Assume no aliasing at all (Intel only): -fno-alias, -fno-fnalias
This allows more compiler optimizations but programmer must assure conformance to the rules!

**1.2.3** What is the meaning of "ivdep"? 

Instructs the compiler to ignore assumed vector dependencies.
Syntax: #pragma ivdep

The ivdep pragma instructs the compiler to ignore assumed vector dependencies. To ensure correct code, the compiler treats an assumed dependence as a proven dependence, which prevents vectorization. This pragma overrides that decision. Use this pragma only when you know that the assumed loop dependencies are safe to ignore.
In addition to the ivdep pragma, the vector pragma can be used to override the efficiency heuristics of the vectorizer.

void ignore_vec_dep(int *a, int k, int c, int m) {
  #pragma ivdep
  for (int i = 0; i < m; i++)
    a[i] = a[i + k] * c; 
}
The loop in this example will not vectorize without the ivdep pragma, since the value of k is not known; vectorization would be illegal if k < 0.
The pragma binds only the for loop contained in current function. This includes a for loop contained in a sub-function called by the current function.


#pragma ivdep 
  for (i=1; i<n; i++) {
    e[ix[2][i]] = e[ix[2][i]]+1.0;
    e[ix[3][i]] = e[ix[3][i]]+2.0; 
}
This loop requires the parallel option in addition to the ivdep pragma to indicate there is no loop-carried dependencies.

#pragma ivdep 
  for (j=0; j<n; j++) { a[b[j]] = a[b[j]] + 1; }
This loop requires the parallel option in addition to the ivdep pragma to ensure there is no loop-carried dependency for the store into a().


**1.2.4** What is the meaning of "-xCORE-AVX512"? enables Intel® AVX-512

CORE-AVX512 is the minimum set of AVX512 instructions included in all AVX512 implementations
-xCORE-AVX512: use this option to generate AVX-512F, AVX-512CD, AVX-512BW, AVX-512DQ and AVX-512VL.

You need to add an additional option along with -xCORE-AVX512.
-xCore-AVX512 -qopt-zmm-usage=high
By using this special new flag [Q/q]opt-zmm-usage=high (which should be used in conjunction with -xCORE-AVX512), you'll get full AVX-512 ISA with 512-bits-wide operands.

https://www.intel.com/content/www/us/en/developer/articles/technical/tuning-simd-vectorization-when-targeting-intel-xeon-processor-scalable-family.html


1.2.5 The Intel compiler provides reports when using "opt-report" option. Remember that only relax_jacobi.c is relevant for the overall performance. What does it tell you, and what does it mean?
Use the following options to trigger opt-report: -qopt-report-annotate -qopt-report-phase=vec,loop. Copy the output for relax_jacobi and explain what it tells you.

opt-report : generate an optimization report to stderr.
opt-report: -qopt-report-annotate -qopt-report-phase=vec,loop

**1.2.6** Is the code vectorized by the compiler? 

**1.2.7** What does it mean by -qopt-zmm-usage?

Syntax
Linux:
-qopt-zmm-usage=keyword

Arguments
keyword - Specifies the level of zmm registers usage. Possible values are:
low - Tells the compiler that the compiled program is unlikely to benefit from zmm registers usage. It specifies that the compiler should avoid using zmm registers unless it can prove the gain from their usage.
high - Tells the compiler to generate zmm code without restrictions.

Default - varies
The default is low when you specify [Q]xCORE-AVX512.
The default is high when you specify [Q]xCOMMON-AVX512

This option may provide better code optimization for Intel® processors that are on the Intel® microarchitecture formerly code-named Skylake.
This option defines a level of zmm registers usage. The low setting causes the compiler to generate code with zmm registers very carefully, only when the gain from their usage is proven. The high setting causes the compiler to use much less restrictive heuristics for zmm code generation.
It is not always easy to predict whether the high or the low setting will yield better performance. Programs that enjoy high performance gains from the use of xmm or ymm registers may expect performance improvement by moving to use zmm registers. However, some programs that use zmm registers may not gain as much or may even lose performance. We recommend that you try both option values to measure the performance of your programs.
This option is ignored if you do not specify an option that enables Intel® AVX-512, such as [Q]xCORE-AVX512 or option [Q]xCOMMON-AVX512.
This option has no effect on loops that use pragma omp simd simdlen(n) or on functions that are generated by vector specifications specific to CORE-AVX512.

**1.2.8** What is the performance result of these options. Present a graph as outlined in the meeting.

**1.2.9** Is there any performance variability you found as presented in the SuperMUC NG presentation.
